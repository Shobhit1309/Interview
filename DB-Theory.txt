---------------
important sqls:
---------------
---------
--sqls:
---------

-- list of employees with same sal
select * from emp e1 inner join emp e2 on e1.sal=e2.sal and e1.empno <> e2.empno;

-- get the dept no which is in employee but not in dept table
select * from emp e left join dept d on e.deptno=d.deptno where d.deptno is null

-- get 3rd highest salary ( for 3rd lowest salary reverse the inner condition )
select * from emp e where 3=(select count(sal) from emp e1 where e1.sal > e.sal)

-- max sal based on deptno
select max(sal),deptno from emp group by deptno;

-- emp name starts with vowels
SELECT *
  FROM emp
 WHERE    UPPER (ename) LIKE 'A%'
       OR UPPER (ename) LIKE 'E%'
       OR UPPER (ename) LIKE 'I%'
       OR UPPER (ename) LIKE 'O%'
       OR UPPER (ename) LIKE '%U';

-- get 4th row
select * from (
select rownum as rownum_id,a.* from emp  a ) where rownum_id=4

-- top 3 sal
select * from (
select sal from emp  order by 1 desc) where rownum <4 


-- odd rows ( for even rows outer condition should be rownums=0
select * from (
select mod(rownum,2) as rownums,a.* from emp a ) where rownums=1



------- Theory and concepts -------

dml(data manipulation language)
ddl(data definition language)
tcl(transaction control language)
dcl(data control language)
drl(data retrival language)

dml-insert,update,delete,insert all,merge(insert+update),explain plan
ddl-create,alter,drop,truncate,purge,flashback,rename
tcl-commit,rollback,savepoint
dcl-grant,revoke
drl-select,select for update(row level locking)


RDBMS-data consistency(correct data across database and no one can alter data who is not authenticated)
      data redundency(avoid duplication of data)
      data integrety (valid and correct data(ex: sal must be stored in a number field and name in char field))
ER diagram:
EMP(entity)---sal,empno,name(relationship)(attributes of entity)

Three tier arch:
1)external(ui-sql* +  check and validated the user and allow to fire queries)
2)conceptual(actual execution of queries based on CBO)
3)internal(database which stores the data)


version 5: introduced sql * +
version 6: hot backup,row level locking,plsql
version 10: grid computing
version 11: CBO(cost based optimizer--based on cost(time) and resources(memory) it executes the query)
	    Automatic tuning for SGA and PGA(system global area,provided global area)

SQL(seequel)- structured query language 
            - non procedural language as we dont use functions
            - not an oops language as it does not support inheritance
Interface level commands like set pagesize,set linesize, quit etc will be executed @ SQL* +
Other statements like select--,delete,--update etc are executed by Oracle engine.

Types of errors:
1)sql error- ORA -3542,ORA -10002 etc
2)plslq error- PLS -8711 etc
3)*+ error - SP2 -786 etc


spaces occupied by the datatypes:
1)char- 2000bytes
2)varchar- 2000 bytes(occupy memory for null values)
3)varchar2- 4000 bytes(doesnot occupy memory for null values)
4)date - 7 bytes
5)number - ??

breakage of database(Ascending order):
1)datablocks(8 kb)
2)extents
3)segments
4)tablespaces
5)database

Datawarehousing:
-----------------
-----------------

Types of schemas:
-------------------
1) star schema 
2) snowflake schema

in star schema we have a fact table which is surrounded by the dimension tables and all the dimension table's 
primary keys are used as forien keys in fact table , where as in snowflake schema which is similar to star schema but the dimension tables are further linked to other dimension tables example : fund table is linked to shareclass table
and holding is using fund id so holding is fact and dimension tables are fund and shareclass.

star schema are mostly used in datamarts where as snowflask schemas are used in datawarehousing.

Datamarts are the subsets of Datawarehousing like a table which containts employee details of all employees
which works in tech , admin , finance but if we collect only the employees and its tables which work only
in tech then the subset is called as datamart.

To get data from star schema less joins will be used where as from snowflake schemas high number of joins  are used.


SCD:
----
Slow changing dimensions- Dimensions which change slowly over the time rather than changing daily is called 
as slow changing dimension.There are 4 types of SCD:
1) SCD1 - in this we update the data in table in case any change comes in a data. We don't maintain history.
2) SCD2 - in this we keep history of change by insert a new record as active record and deactivated the old record.
This maintains history and current data both in the table and for OLAP reporting purpose we can take history data
from the same table and for OLTP processing the same table can be used with the current data.
3) SCD3 - in this we have columns for capturing the old value and new values of the data . We insert newly modified data
with value in new columns and old column as null and for old data values are in old columns and new column is null.
4) SCD6 - the combination of SCD1,2,3 forms SCD6 .


Dimensions:
-----------
There are various entities in a data which creates meaning to a data are dimensions and the measurement  data becomes fact.So in order to get the insight out of data we have various entities in data ex:
for getting details about the sales we have 
-customer details like id , name , address
-product details like id , name, type
-date details like order_date, ship_date,recieved_date

so we have customer dimension, product dimension and date dimension and further there are various attributes in these dimensions.Here sales are the facts and dimensions are the entities which gives meaning to sales.
Dimension answers the seven questions like who , what, why, how, when, where, howmanys

Dimension table consists of primary keys which are used as foreign keys in fact table.

Fact:
-----
A fact is a table containing measure of dimensions.It contains dimension keys (forein keys) and a measure.
here fact tables are holding , transaction , valuation and dimension tables are fund , security.
In fact tables the data is present at the lowest grain like everyday transaction, everyday holding , everyday details of each security etc. where an in dimension table we have reference data which can be linked to fact tables to give meaning to dimension. A list of security is making no significance but the details i.e measure of securities using holding transaction is making sense.



Confirmed dimensions:
---------------------
The dimension tables which are shared by more than 1 fact table is called as confirmed dimensions.
ex: Fund is shared between holding , transaction , valuation. 

Factless fact tables:
---------------------
when a fact is not captured in a fact table and the dimensions cannot measure the data then it is called as factless
fact table. ex: for a day no holding and transaction is captured for a security so no measure is done .. for all
transaction 0 is present in fact table which is not giving the details and hence they are factless fact tables

Confirmed facts and Confirmed dimensions:
-----------------------------------------
The dimensions which can be used by more than 1 fact tables are called confirmed dimensions like security is used by holding and transaction table.

The facts which can be used in multiple datamart with combination of other fact tables are called as confirmed fact tables.
ex: in a datamart there are few fact table and if those tables are used with holding fact tables then holding fact table
is called as confirmed facts.

Aggrigate tables:
------------------
The tables which are used to store the aggrigated information of data is called aggrigate tables.
example : for a security if the total sales for a week is stored in another table then it is called as aggrigated tables.
Aggrigated tables helps in increasing the performance of sqls as we don't need to aggrigate and then show the data in reporting or use this data after calculation. We can simple fetch the aggrigated data.

Data mart :
----------
Data mart is a subset of Datawarehouse and it is specially build for specific departments where as datawarehouse is a collection of all types of data.

Data mining:
------------
It is used to analyse the data present in dataware house and get insights from the data. The analysis is done in order to get hidden details about data. Data mining algorithms are used for predictive analysis.

When we use data normalization:
-------------------------------
For transactional system we use data normalization to speedup to reduce data redundency and it actually solves the problem 
of insert update delete which may accidently corrupt the entire data.Ex: if a student details and hod details are present in the same table then for each new student the hod details and branch will be same. If by mistake hod detail of 1 row is 
updated then the remaining data will be incorrect.Also if a detail needs to be changed the update will run on all the rows which is very costly and hence to respond quick to applications in OLTP system we split table into different tables basedd on functional dependencies.

Where as in datawarehouse our main focus is to do reporting and data analyse and we don't perform DMLs. We follow SCD-2 and above so there is no need to make seperate tables and hence everything is stored in a single table.

The databases on which transactions happen frequently are called as OLTP(online transaction processing)
and the once which stores history and reporting happens on it are called as OLAP(online analytical processing).

Time series data is a data changing with time ( fixed duration of time ).
ex: stock prices of a fund , turnover rates each month etc.

Datalake is like a container which consists of various types of data coming from various sources like RDBMS unstructured data, log files etc. which is blended properly so that any system can connect and consume the data for analytics , data mining etc.

Fact tables are those which consists of dependent details which are of no use without the reference data and dimension tables are those which contains reference data.

ex: Fund data and security data are dimension tables and holdings , trades , valuations are fact tables.

If we have various facts and dimensions tables in an OLTP system and if we want to get complex details
from the stored data then various joins are required which is not good in OLTP system as the transaction data needs to be accessed very frequently and using many joins can increase cpu cost which might bring performance issues, thus in dataware house we perform the below tasks while designing schemas:

1) club dimension tables to create less tables if possible.

2) use star schema which is basically having a fact table in the center surrounded by dimension tables and all the primary keys of dimension tables will be foreign key of fact table.

Using this the joins will be reduced and the data can be fetched more effectively and fast.

3) In some cases when we have hugh dimension tables then denormalizing multiple tables into a single table is not a good idea. In this case we normalize the single dimension table into multiple dimension tables and then adding the primary keys into fact table as foreign keys.This creates snowflake schema which is a varient of star schema.


Triggers:
--------
These are the process which we create in database which executes when an event is occured. There are different types of triggers. They are:

1) DML triggers -- when insert update or delete happens on tables.

2) DDL triggers -- when alter,truncate,drop

3) System event triggers -- when a system event happens like startup,shutdown,logon,logoff of database.

4) Instead of triggers -- these are written on top of views and when an update,insert,delete happens on 
the view it fires.

5)Compound triggers -- these are same as DML triggers but the only difference is that all the events like insert , update , delete can be written in a single block.


Before and After triggers : before triggering event or after triggering event.

Row level triggers: If a row is affected then row level trigger will fire
statement level trigger : If a statement is execute but it does not affect row then also statement trigger 
will fire.

update table set name='aaa' where name='bbb' 
if update runs and it updates a row then statement level and row level will fire
else statement level will fire.


Difference between delete and truncate:
----------------------------------------

Delete is a DML command and Truncate is a DDL command.
Delete needs commit and works with where condition where as truncate is autocommit and does not runs with where condition.
Triggers can be created with Delete but no triggers can be created on truncate action.
Delete is slower because there is undo segment which is used for rollback where as in Truncate no undo segment is used.
Delete does not reduce the table size where as Truncate makes the size to 0. This is because the table size goes to the freesize block and freesize can be used by other tables.

Difference between procedure and functions:
-------------------------------------------
Procedures are plsql blocks which can return value using the in out or the out parameter where as function
can return value using the return keyword.

Procedures cannot be executed using select statements
where as functions can be executed using select statements.Functions with DML statements cannot be called using select statement , it can be called using autonomous transactions or child transactions written in the function.

Return keyword used in the procedure is used to exit from the procedure based on a condition where as return keyword in function is used to return a value from function.

NOTE: function can be compiled without return statement
but when executed it will give error.

NOTE: from 12.1 onwards procedure and function can be created using with clause , it does not need plsql block. Just like lambda functions in python.

NVL2 function:
--------------
select nvl2('a','b','c') from dual

if 1st argument is null then it will return 3rd 
and if 1st argument is not null then it will return 2nd

it is opposite to nvl.

NULLIF function:
----------------
select nullif('a','a') from dual -- output is null

if both the args are same then it will return null
if both the args are different then it will return first arg.

select nullif('a','b') from dual -- output is 'a'

COALESCE function:
-------------------
select coalesce(NULL,'b','c','d') from dual;

oracle checks the first value which is not null and returns the same.
ex: in the above sql it will return 'b'

select coalesce(NULL,NULL,'c','d') from dual;
in the above sql it will return 'c'


Difference between Rank and dense Rank:
----------------------------------------

Rank and Dense Rank function is used to get the rank of the record
based on the column which must be sorted.The difference between both is :
consider an example:

sal rank   dense rank
--- ----  ------------
100	 1			1
200	 2			2
200	 2			2
300  4 (case1)	3 (case2)
400  5			4
400  5			4
500  7			5

Here in rank the last rank value must be equals to total number of records
In dense rank the last rank value will not be equals to total number of records

In rank the value is skipped if a records repeat like case1 where as in 
dense rank the values will not be skipped.

sqls:
****

SELECT empno,
       ename,
       deptno,
       sal,
       RANK () OVER (ORDER BY sal)           AS RANK_COL,
       DENSE_RANK () OVER (ORDER BY sal)     AS DENSE_RANK_COL
  FROM emp;
  
  						rank    dense_rank
  						------  --------	
7369	SMITH	90	800	    1	1
7900	JAMES	30	950	    2	2
7876	ADAMS	20	1100	3	3
7521	WARD	30	1250	4	4
7654	MARTIN	30	1250	4	4
7934	MILLER	10	1300	6	5
7844	TURNER	30	1500	7	6
7499	ALLEN	30	1600	8	7
7782	CLARK	10	2450	9	8
7698	BLAKE	30	2850	10	9
7566	JONES	20	2975	11	10
7788	SCOTT	20	3000	12	11
7902	FORD	20	3000	12	11
7839	KING	10	5000	14	12

Get the distinct records without distinct:
------------------------------------------
select deptno from emp union select deptno from emp; -- here union will show distinct records
select deptno from emp intersect select deptno from emp; -- here intersect gives the common records
select * from (
select deptno,row_number() over (partition by deptno order by deptno) r from emp)

10	1
10	2
10	3
20	1
20	2
20	3
20	4
30	1
30	2
30	3
30	4
30	5
30	6

after applying where r=1 we get unique records

 Generate 10 numbers without loop:
 ---------------------------------
 select rownum from dual connect by level<11;


Difference between replace and translate function:
--------------------------------------------------

select replace('welcome to oracle','oracle','python') from dual; -- welcome to python

select translate('welcome to oracle','oracle','python') from dual; -- wnohpmn tp python

replace function is used to replace a part of string with the 3rd parameter where as translate function 
is used to replace each character mentioned in 2nd parameter with the characters of 3rd parameter.

Views and Mviews:
-----------------
Views does not hold its own data. It is build on top of table.
DMLs can be executed on simple view with no joins and no aggrigations where as DMLs are not allowed on 
complex views with joins and aggrigations.


DMLs and DDLs in procedure and function:
----------------------------------------
DMLs can not be executed in a function or procedure directly. It can be executed if a function has pragma autonomous transaction.

DDLs can not be executed in a function or procedure directly. It can be executed using execute immediate
block and using pragma autonomous tramsaction.


Can a function return more than 1 value:
-----------------------------------------
Yes a function can return more than 1 value. Using the return statement and OUT parameter in a function, 
it can return more than 1 value. This cannot be called via select statement , instead it can be called via begin end block.

View and Materialized view:
-----------------------------

when ever we access view then it executes the query on the underlying table and gives the result
where as when ever we access mviews then it fetches the data stored in mview. The query will not hit the underlying table. To get the latest data mviews needs to be refreshed where as in views, any changes in the underlying table will be directly reflected in view.

Types of Mview refresh:
-----------------------
There are 3 types of refresh :

1) Complete refresh
2) Fast refresh (MV logs are used to capture incremental data)
3) Force refresh

1) In this refresh we truncate the mv and reload the data from the base table.
2) In this we only perform incremental refresh i.e the records which are changed will be updated in MV.
3) In this we first go for fast refresh but in case of any errors (MV log is corrupted) the refresh switches to complete refresh.


Which one is faster Union or Union all:
----------------------------------------
Union is faster than Union all because union removes duplicate and sort the input which takes more time where as Union all 
gives all the data.

% rowtype in sql:
-----------------
If we want to fetch multiple columns for a single row so we can use %rowtype. It's like collections or varrays but 
it can only hold 1 row.Below is an example

DECLARE
    l_data   emp%ROWTYPE;
BEGIN
    SELECT *
      INTO l_data
      FROM emp
     WHERE empno = 7369;

    DBMS_OUTPUT.put_line (l_data.empno || '_' || l_data.ename);
END;


Types of indexes:
-----------------
1) Btree index
2) Bitmap index -- created on the columns which has very less distinct values
3) Function based index -- 
if we create normal index on a column and using a function so oracle will not consider index
hence we create function based index like upper(ename) is a function so we create index and mention 
function which will be used on column.
4) Simple index -- index created on 1 column 
5) Composite index -- index created on mutliple columns
** the address is stored in rowid column **

Data dictionary views:
---------------------

select * from v$sql -- it stored which sql is executed and using this we get sqlid
select * from v$sql_plan where sqlid = <sqlid from above> -- using this we get plan picked by sql

Cardinality : distinct value in a column/ total number of rows
Cost : 

Type of scans in index:
-----------------------
1) Range scan - normal indexes created on the table performs range scan i.e it search it find the value and continue 
so that it does not miss the value due to duplicates.. example 2 employee with same name so it will search 
till it finds different employee name in btree structure.

2) Unique scan - when we create unique index then this will perform.

3) Index full scan - oracle goes to index segment to perform aggrigation instead on table column.
select max(sal) from emp; -- here sal column consists of index.

Disadvantages of Btree-- The dml performance will degrade because oracle needs to rearrange the columns which have
btree indexes which will degrade the performance. Also Bitmap is faster when we have few distinct values.

Bitmap index:
-------------
This is created when we have less distinct values in a column. Ex: Male , Female details in Gender column.
Oracle internally creates a bitmap matrix with columns Male and Female and values as 1 and 0. 1 for avaible 
and 0 for not available ( if male is available in a row then 1 else 0 , same with female)

Disadvantage is when DML acts on the bitmap columns then oracle locks the rows having same value.
update table set gender='M'; -- here all the rows with M will be locked in bitmap and takes large resource.


SQL loader:
------------
This is a utility provided by sql to load files in database.

All the details are mentioned in control file
1) source details
2) target details
3) conditions which will filter the data while loading (discarded records)
4) format of file (csv tab etc)

LOG FILE : It will log the details of sql loader
DISCARDED FILE : It will keep the discarded records
BAD FILE : It will keep the failed records


Varray , Nested and Associative Array:
--------------------------------------

  no of elements:
  --------------
  varray : Maximum elements in an array is defined at the defination type so we cannot extend
  Nested : we can extend with any number of elements
  Associative : we can extend with any number of elements

  Index:
  --------------
  varray : index will be integer
  Nested : index will be integer
  Associative : index can be varchar2 or integer


  Extend keyword:
  --------------
  varray : Applicable
  Nested : Applicable
  Associative : NA

Exceptions:
-------------
1) System defined exceptions which we can use directly in exception block like ZERO_DIVIDE
exception when ZERO_DIVIDE then ------

2) User defined exceptions which we create our own custom message.
we take a exception type of variable like v_exp exception ; and then in the code we do raise v_exp.
in the excception block we do 
exception when v_exp then --------

3) To handle an unnamed exception like precision error etc. we use pragma exception_init

4) For user defined error we can also use raise_application_error, this will give more standard errror messages.



Cursors:
----------
cursor declare
cursor open
cursor loop
cursor fetch
cursor close

cursor stores the selected data in memory where as nested tables stored the data in objects.

Types of cursors:
------------------
1) Implicit cursors -- oracle internally create cursors while executing select statement.
2) Explicit cursors -- we create cursors . There are types of Explicit cursors. They are:
  -- NAMED
  -- REF
    >>> Strong typed
    >>> Weak typed
  -- FOR

Cursor attributes:
-------------------
>> isopen -- is open or not 
>> found -- record found in cursor 
>> notfound -- record not found in cursor
>> rowcount -- how many records fetched from cursor

Parameterized cursors are those which takes parameter and based on that parameter it selects data from table.
ex: for dept 10 and 20 we have declared a parameterized cursor and in program while opening cursor we will 
pass parameters.

FOR cursor is used when we don't want to declare cursor.It's like for loop :

begin
for i in (select ename,dept from emp)
loop
dbms_output.put_line(i.ename||'-'||i.dept)
end loop;
end;


REF cursor:
----------
It is a special type of cursor in which same cursor name can be used for different sqls.
We define the cursor as ref cursor but does not provide the query.
When we open the cursor then we provide the select statement.
Dynamic Sqls can also be assigned to ref cursor

Strong ref cursor:
----------------- 
The ref cursor which has return type are called strong ref cursor. ex: a ref cursor which is defined is only intended 
to return the rows from emp not from dept then we add return keyword emp%rowtype . Now if i use the same ref cursor 
for dept , it will give error.

Weak ref cursor:
---------------
When we don't give return type.


sys ref cursor:
---------------
function ---> returns sysrefcursor ---> can be used and assigned to sysrefcursor of another begin end block.

Cursor FOR UPDATE clause:
-------------------------
a program is executing on 1 session which is using cursor to get the data and based on that data 
update is running on table. Support before running update delete has run in some other session , 
so update will not update the records .. which is a flaw. To lock the transaction in which a cursor
based program is running which is also updating , we use FOR UPDATE clause while declaring cursor.
This will lock the transaction and no one can run dml on the table.


Difference between cursor and collections:
--------------------------------------------
Collections are generally used for bulk processing. In cursors the data is processed sequentially. Using collections we can  process all the data in one go.It can be passed as parameter to sub programs like lists to another program.


Bulk collect advantages:
-------------------------
While running a plsql program with plsql context and sql context there is context switching happens. Plsql engine will process plsql part where as sql engine will process sql part. To reduce context switching and to speed up the process, 
bulk collect is used. Instead of row by row processing the complete sql data will be stored in a variable and then we can process.

Bulk collect can be used with table datatypes like nested table varrays and associated table and can also be used in explicit cursors.

fetch the result of cursor into a table data type using bulk collect into clause. 

This will reduce context switching of cursor to store rows in table type.

bulk collect can be used with cursors
bulk collect can be used with table data types


FOR ALL (bulk dml operations)
-------
select into table variable.
for all i in 1..table variable.count
insert into table values table variable(i)
save exception
at last get failed records using save exception and store in table.
If we want to perform bulk dml operations without context switching , we can do using FOR ALL.
In FOR ALL we can apply save exceptions for the failed rows. The dml will not fail if any record failed.
In FOR ALL we can only use 1 dml at a time.. like only insert, only update , only delete.
